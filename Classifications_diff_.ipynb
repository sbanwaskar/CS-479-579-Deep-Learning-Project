{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifications-diff-.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbanwaskar/CS-479-579-Deep-Learning-Project/blob/main/Classifications_diff_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozm8M8Tr__Y2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as st\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer \n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB7dRpddAC8j"
      },
      "source": [
        "# #data import\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "!wget https://www.dropbox.com/s/xkozscpcryu871h/labels_1540_4classes_icmla_21.pck\n",
        "!wget https://www.dropbox.com/s/ovt8g99wjcz1mvr/mvts_1540_icmla_21.pck\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRLzevlnAEvH"
      },
      "source": [
        "#reading pickle files\n",
        "def load(file_name):\n",
        "    with open(file_name, 'rb') as fp:\n",
        "        obj = pickle.load(fp)\n",
        "    return obj\n",
        "\n",
        "\n",
        "Sampled_inputs=load(\"mvts_1540_icmla_21.pck\")\n",
        "\n",
        "\n",
        "Sampled_labels=load(\"labels_1540_4classes_icmla_21.pck\")    \n",
        "\n",
        "temp=Sampled_inputs[0]\n",
        "print(temp)\n",
        "df = pd.DataFrame(temp)\n",
        "trainData=Sampled_inputs\n",
        "trainLabel=Sampled_labels\n",
        "print(\"trainData.shape: \",trainData.shape)\n",
        "print(\"trainLebel.shape: \",trainLabel.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#standardization/z normalization of the univaraite time series\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "npArrays=[]\n",
        "for l in range(0, len(trainData)):\n",
        "  trainData_std = sc.fit_transform(trainData[l])\n",
        "  #trainData_std = trainData_std.astype(np.float64)\n",
        "  #print(type(trainData_std[0][0]))\n",
        "  npArrays.append(trainData_std)\n",
        "\n",
        "print(type(npArrays))\n",
        "arr = np.asarray(npArrays)\n",
        "print(type(arr))\n",
        "trainData=arr\n",
        "print(\"trainData.shape: \",trainData.shape)\n",
        "print(type(trainData))\n",
        "print(\"trainLebel.shape: \",trainLabel.shape)\n",
        "print(type(trainLabel))"
      ],
      "metadata": {
        "id": "SyPv8JBTUVRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P12kQOtsCBzA"
      },
      "source": [
        "Saving all data to csv (different files)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pzz4x5g9Ads3"
      },
      "source": [
        "def time_series(trainData):\n",
        "\n",
        "  rowList = []\n",
        "#Here we parse through the entire trainData matrix and append eveything to the row list.\n",
        "  for i in range(0, len(trainData)):\n",
        "\n",
        "    for j in range(0, len(trainData[i])):\n",
        "      \n",
        "      rowList.append(trainData[i][j])\n",
        "  # print(len(rowList))\n",
        "  return rowList\n",
        "\n",
        "def time_series1(rowList):\n",
        "  rows, cols = (len(rowList), len(rowList[0])-1)\n",
        "\n",
        "# here we declare a row list to which we append the values for all the 8 parameters.\n",
        "  rowList1 = [[0 for i in range(cols)] for j in range(rows)]\n",
        "#Here we parse through the entire trainData matrix and append eveything to the row list.\n",
        "  for i in range (0, len(rowList)):\n",
        "\n",
        "    for j in range(0,len(rowList[i])-1):\n",
        "\n",
        "      rowList1[i][j] = rowList[i][j+1] - rowList[i][j]\n",
        "  return rowList1\n",
        "\n",
        "def get_mean_std(rowList):\n",
        "  Mean = np.array(rowList).mean(1)\n",
        "  means = np.reshape(Mean, (1540, 33))\n",
        "  standardDeviation = np.array(rowList1).std(1)\n",
        "  stdDev = np.reshape(standardDeviation, (1540, 33))\n",
        "  return means,stdDev\n",
        "\n",
        "def get_skew_and_kurtosis(rowList):\n",
        "  df = pd.DataFrame(rowList)\n",
        "  skew_df = df.skew(1)\n",
        "  kurtosis_df = df.kurtosis(1)\n",
        "  skew_rs = np.reshape(skew_df.values,(1540, 33))\n",
        "  kurtosis_rs = np.reshape(kurtosis_df.values,(1540, 33))\n",
        "  return skew_rs , kurtosis_rs\n",
        "\n",
        "def get_all_params(rowList):\n",
        "  mean , std_dev =get_mean_std(rowList)\n",
        "  skew , kurtosis = get_skew_and_kurtosis(rowList)\n",
        "  return mean,std_dev, skew,kurtosis\n",
        "\n",
        "rowList = time_series(trainData)\n",
        "rowList1 = time_series1(rowList)\n",
        "\n",
        "mean,std_dev,skew , kurtosis = get_all_params(rowList)\n",
        "mean1,std_dev1,skew1 , kurtosis1 = get_all_params(rowList1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtprlAu_AfnM"
      },
      "source": [
        "all_params = [mean, std_dev,skew , kurtosis,mean1,std_dev1,skew1 , kurtosis1]\n",
        "all_param_names = ['mean','std_dev','skew','kurtosis','mean1','std_dev1','skew1','kurtosis1']\n",
        "\n",
        "\n",
        "## Method 1 : All Data in one csv please follow all_param_names to check the sequencing of data\n",
        "# write\n",
        "all_arrays_list = [a for a in all_params]\n",
        "all_arrays_np = np.array(all_arrays_list).reshape(1540*8 , 33)\n",
        "np.savetxt(\"all_params_data.csv\", all_arrays_np, delimiter=\",\")\n",
        "# read\n",
        "data_read_back = np.loadtxt('all_params_data.csv',delimiter=',')\n",
        "all_param_names = ['mean','std_dev','skew','kurtosis','mean1','std_dev1','skew1','kurtosis1']\n",
        "all_param_data = data_read_back.reshape(8, 1540 , 33)\n",
        "\n",
        "\n",
        "## Method 2 : All in one excel with different sheets (more organized, better when checking for issues in data by reading the excel manually)\n",
        "# write-only\n",
        "df_list = [pd.DataFrame(val) for val in all_params] \n",
        "filename = 'all_params_excel'\n",
        "writer = pd.ExcelWriter(filename+'.xlsx')\n",
        "for i, df in enumerate(df_list):\n",
        "    df.to_excel(writer,sheet_name=all_param_names[i])\n",
        "writer.save() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkDIMViIAhnW"
      },
      "source": [
        "# All in one compressed pickle\n",
        "\n",
        "final_data = {'mean':mean,\n",
        "              'std_dev':std_dev,\n",
        "              'skew':skew,\n",
        "              'kurtosis':kurtosis,\n",
        "              'mean1':mean1,\n",
        "              'std_dev1':std_dev1,\n",
        "              'skew1':skew1,\n",
        "              'kurtosis1':kurtosis1}\n",
        "\n",
        "import bz2\n",
        "import pickle\n",
        "import _pickle as cPickle\n",
        "\n",
        "filename = 'all_data_in_one'\n",
        "\n",
        "def decompress_pickle(file):\n",
        "  data = bz2.BZ2File(file, 'rb')\n",
        "  data = cPickle.load(data)\n",
        "  return data\n",
        "\n",
        "def compressed_pickle(title, data):\n",
        "  with bz2.BZ2File(title + '.pbz2', 'w') as f: \n",
        "   cPickle.dump(data, f)\n",
        "\n",
        "compressed_pickle(filename, final_data) \n",
        "data = decompress_pickle(filename+'.pbz2') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TooKlLsTAls2"
      },
      "source": [
        "!zip -r all_data_in_one.zip all_*\n",
        "from google.colab import files\n",
        "files.download('all_data_in_one.zip') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujjUEBjsAqzw"
      },
      "source": [
        "def time_series():\n",
        "\n",
        "  rowList = []\n",
        "#Here we parse through the entire trainData matrix and append eveything to the row list.\n",
        "  for i in range(0, len(trainData)):\n",
        "\n",
        "    for j in range(0, len(trainData[i])):\n",
        "      \n",
        "      rowList.append(trainData[i][j])\n",
        "  # print(len(rowList))\n",
        "  return rowList\n",
        "\n",
        "def time_series1(rowList):\n",
        "  rows, cols = (len(rowList), len(rowList[0])-1)\n",
        "\n",
        "# here we declare a row list to which we append the values for all the 8 parameters.\n",
        "  rowList1 = [[0 for i in range(cols)] for j in range(rows)]\n",
        "#Here we parse through the entire trainData matrix and append eveything to the row list.\n",
        "  for i in range (0, len(rowList)):\n",
        "\n",
        "    for j in range(0,len(rowList[i])-1):\n",
        "\n",
        "      rowList1[i][j] = rowList[i][j+1] - rowList[i][j]\n",
        "  return rowList1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv4NftybAsXt"
      },
      "source": [
        "def mean():\n",
        "  Mean = np.array(rowList).mean(1)\n",
        "  means = np.reshape(Mean, (1540, 33))\n",
        "  df = pd.DataFrame(means)\n",
        "  print(means)\n",
        "  df.to_csv(\"mean.csv\")\n",
        "def deviation():\n",
        "  standardDeviation = np.array(rowList).std(1)\n",
        "  stdDev = np.reshape(standardDeviation, (1540, 33))\n",
        "  DF = pd.DataFrame(stdDev)\n",
        "  print(\"info for deviation\")\n",
        "  print(DF.info)\n",
        "  DF.to_csv(\"standardDeviation.csv\")\n",
        "def  skew():\n",
        "  df = pd.DataFrame(rowList)\n",
        "  skewValue = np.array(df.skew(1))\n",
        "  skews = np.reshape(skewValue, (1540, 33))\n",
        "  df = pd.DataFrame(skews)\n",
        "  print(\"info for skew\")\n",
        "  print(df.info)\n",
        "  df.to_csv(\"skew.csv\")\n",
        "def kurt():\n",
        "  df = pd.DataFrame(rowList)\n",
        "  kurtosisValue = np.array(df.kurt(1))\n",
        "  kurts = np.reshape(kurtosisValue, (1540, 33))\n",
        "  df = pd.DataFrame(kurts)\n",
        "  print(\"info for kurtsosis\")\n",
        "  print(df.info)\n",
        "  df.to_csv(\"kurt.csv\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JG1Y3YDAuC9"
      },
      "source": [
        "def mean():\n",
        "  Mean = np.array(rowList).mean(1)\n",
        "  means = np.reshape(Mean, (1540, 33))\n",
        "  df = pd.DataFrame(means)\n",
        "  print(means)\n",
        "  df.to_csv(\"mean.csv\")\n",
        "def deviation():\n",
        "  standardDeviation = np.array(rowList).std(1)\n",
        "  stdDev = np.reshape(standardDeviation, (1540, 33))\n",
        "  DF = pd.DataFrame(stdDev)\n",
        "  print(\"info for deviation\")\n",
        "  print(DF.info)\n",
        "  DF.to_csv(\"standardDeviation.csv\")\n",
        "def  skew():\n",
        "  df = pd.DataFrame(rowList)\n",
        "  skewValue = np.array(df.skew(1))\n",
        "  skews = np.reshape(skewValue, (1540, 33))\n",
        "  df = pd.DataFrame(skews)\n",
        "  print(\"info for skew\")\n",
        "  print(df.info)\n",
        "  df.to_csv(\"skew.csv\")\n",
        "def kurt():\n",
        "  df = pd.DataFrame(rowList)\n",
        "  kurtosisValue = np.array(df.kurt(1))\n",
        "  kurts = np.reshape(kurtosisValue, (1540, 33))\n",
        "  df = pd.DataFrame(kurts)\n",
        "  print(\"info for kurtsosis\")\n",
        "  print(df.info)\n",
        "  df.to_csv(\"kurt.csv\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpUSnbHUAvkB"
      },
      "source": [
        "def time_series1(rowList):\n",
        "  rows, cols = (len(rowList), len(rowList[0])-1)\n",
        "\n",
        "# here we declare a row list to which we append the values for all the 8 parameters.\n",
        "  rowList1 = [[0 for i in range(cols)] for j in range(rows)]\n",
        "#Here we parse through the entire trainData matrix and append eveything to the row list.\n",
        "  for i in range (0, len(rowList)):\n",
        "\n",
        "    for j in range(0,len(rowList[i])-1):\n",
        "\n",
        "      rowList1[i][j] = rowList[i][j+1] - rowList[i][j]\n",
        "\n",
        "  return rowList1\n",
        "    #print(len(rowList))\n",
        "# the size of the row list is 1540*33 = 50820.\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyDg9imaAy2q"
      },
      "source": [
        "Mean = np.array(rowList1).mean(1)\n",
        "means = np.reshape(Mean, (1540, 33))\n",
        "df = pd.DataFrame(means)\n",
        "df.to_csv(\"mean.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2b8TeZWA0eP"
      },
      "source": [
        "def mean1():\n",
        "  Mean = np.array(rowList1).mean(1)\n",
        "  means = np.reshape(Mean, (1540, 33))\n",
        "  df = pd.DataFrame(means)\n",
        "  df.to_csv(\"mean1.csv\")\n",
        "def deviation1():\n",
        "  standardDeviation = np.array(rowList1).std(1)\n",
        "  stdDev = np.reshape(standardDeviation, (1540, 33))\n",
        "  DF = pd.DataFrame(stdDev)\n",
        "  DF.to_csv(\"standardDeviation1.csv\")\n",
        "def skew1():\n",
        "  df = pd.DataFrame(rowList1)\n",
        "  skewValue = np.array(df.skew(1))\n",
        "\n",
        "#print(len(skewValue))\n",
        "  skews = np.reshape(skewValue, (1540, 33))\n",
        "  df = pd.DataFrame(skews)\n",
        "  df.to_csv(\"skew1.csv\")\n",
        "def kurt1():\n",
        "  df = pd.DataFrame(rowList1)\n",
        "  kurtosisValue = np.array(df.kurt(1))\n",
        "#print(len(skewVale))\n",
        "  kurts = np.reshape(kurtosisValue, (1540, 33))\n",
        "  df = pd.DataFrame(kurts)\n",
        "  df.to_csv(\"kurt1.csv\")\n",
        "#size of kurtosis is 1540*33\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGZz3g5mJO8q"
      },
      "source": [
        "def main_mean():\n",
        " df = pd.read_csv(\"all_params_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYFmd5agK4wn"
      },
      "source": [
        "main_mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import loguniform\n",
        "from pandas import read_csv\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "metadata": {
        "id": "GE1ond-SW7xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJK6IgaQ_7u1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c937e85-af84-40ac-f6c7-956e5fc3f2e3"
      },
      "source": [
        "#Here change the name of the file in [''] to check accuracy of required file\n",
        "\t# ['mean'] ['std_dev'] ['skew'] ['kurtosis'] ['mean1'] ['std_dev1'] ['skew1'] ['kurtosis']\n",
        "X = decompress_pickle('all_data_in_one.pbz2')['kurtosis1']\n",
        "Y = trainLabel\n",
        "X.shape,Y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1540, 33), (1540,))"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bh2GtYFIda0"
      },
      "source": [
        "**KNN Classification**\n",
        "**/SVM**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rurXBRGkEuOB"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from xgboost import XGBClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2A0sJfrP7eo"
      },
      "source": [
        " from sklearn.model_selection import train_test_split\n",
        " X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30)\n",
        " from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CF7nNixFJMd"
      },
      "source": [
        "#using polynomial kernel\n",
        "from sklearn.svm import SVC\n",
        "svclassifier_polynomail = SVC(kernel='poly', degree=8)\n",
        "svclassifier_polynomail.fit(X_train, Y_train)\n",
        "#Y_pred = svclassifier.predict(X_test)\n",
        "Y_pred = svclassifier_polynomail.predict(X_test)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(Y_test, Y_pred))\n",
        "print(classification_report(Y_test, Y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph6vBkkx80d5"
      },
      "source": [
        " #using gaussian rbf kernel\n",
        "svclassifier_gauss = SVC(kernel='rbf',random_state=0)\n",
        "svclassifier_gauss.fit(X_train, Y_train)\n",
        "Y_pred = svclassifier_gauss.predict(X_test)\n",
        "print(confusion_matrix(Y_test, Y_pred))\n",
        "print(classification_report(Y_test, Y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sruq5KT9UVh"
      },
      "source": [
        "#using sigmoid kernel\n",
        "\n",
        "svclassifier_sigmoid = SVC(kernel='sigmoid',random_state=0)\n",
        "svclassifier_sigmoid.fit(X_train, Y_train)\n",
        "Y_pred = svclassifier_sigmoid.predict(X_test)\n",
        "print(confusion_matrix(Y_test, Y_pred))\n",
        "print(classification_report(Y_test, Y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukj-PqPB9kTe"
      },
      "source": [
        "#using linear kernel\n",
        "svclassifier_linear = SVC(kernel='linear')\n",
        "svclassifier_linear.fit(X_train, Y_train)\n",
        "Y_pred = svclassifier_linear.predict(X_test)\n",
        "print(confusion_matrix(Y_test, Y_pred))\n",
        "print(classification_report(Y_test, Y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.cross_validation import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Function importing Dataset\n",
        "\n",
        "# Function to split the dataset\n",
        "def splitdataset(balance_data):\n",
        "\n",
        "\t# Separating the target variable\n",
        "\t#Here change the name of the file in [''] to check accuracy of required file\n",
        "\t# ['mean'] ['std_dev'] ['skew'] ['kurtosis'] ['mean1'] ['std_dev1'] ['skew1'] ['kurtosis']\n",
        "\tX = decompress_pickle('all_data_in_one.pbz2')['std_dev1']\n",
        "\tY = trainLabel\n",
        "\n",
        "\t# Splitting the dataset into train and test\n",
        "\tX_train, X_test, y_train, y_test = train_test_split(\n",
        "\tX, Y, test_size = 0.3, random_state = 100)\n",
        "\t\n",
        "\treturn X, Y, X_train, X_test, y_train, y_test\n",
        "\t\n",
        "# Function to perform training with giniIndex.\n",
        "def train_using_gini(X_train, X_test, y_train):\n",
        "\n",
        "\t# Creating the classifier object\n",
        "\tclf_gini = DecisionTreeClassifier(criterion = \"gini\",\n",
        "\t\t\trandom_state = 100,max_depth=3, min_samples_leaf=5)\n",
        "\n",
        "\t# Performing training\n",
        "\tclf_gini.fit(X_train, y_train)\n",
        "\treturn clf_gini\n",
        "\t\n",
        "# Function to perform training with entropy.\n",
        "def tarin_using_entropy(X_train, X_test, y_train):\n",
        "\n",
        "\t# Decision tree with entropy\n",
        "\tclf_entropy = DecisionTreeClassifier(\n",
        "\t\t\tcriterion = \"entropy\", random_state = 100,\n",
        "\t\t\tmax_depth = 3, min_samples_leaf = 5)\n",
        "\n",
        "\t# Performing training\n",
        "\tclf_entropy.fit(X_train, y_train)\n",
        "\treturn clf_entropy\n",
        "\n",
        "\n",
        "# Function to make predictions\n",
        "def prediction(X_test, clf_object):\n",
        "\n",
        "\t# Predicton on test with giniIndex\n",
        "\ty_pred = clf_object.predict(X_test)\n",
        "\tprint(\"Predicted values:\")\n",
        "\tprint(y_pred)\n",
        "\treturn y_pred\n",
        "\t\n",
        "# Function to calculate accuracy\n",
        "def cal_accuracy(y_test, y_pred):\n",
        "\t\n",
        "\tprint(\"Confusion Matrix: \",\n",
        "\t\tconfusion_matrix(y_test, y_pred))\n",
        "\t\n",
        "\tprint (\"Accuracy : \",\n",
        "\taccuracy_score(y_test,y_pred)*100)\n",
        "\t\n",
        "\tprint(\"Report : \",\n",
        "\tclassification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "def main():\n",
        "\t\n",
        "  #Here change the name of the file in [''] to check accuracy of required file\n",
        "\t# ['mean'] ['std_dev'] ['skew'] ['kurtosis'] ['mean1'] ['std_dev1'] ['skew1'] ['kurtosis']\n",
        "\tdata = decompress_pickle('all_data_in_one.pbz2')['std_dev1']\n",
        "\tX, Y, X_train, X_test, y_train, y_test = splitdataset(data)\n",
        "\tclf_gini = train_using_gini(X_train, X_test, y_train)\n",
        "\tclf_entropy = tarin_using_entropy(X_train, X_test, y_train)\n",
        "\t\n",
        "\t# Operational Phase\n",
        "\tprint(\"Results Using Gini Index:\")\n",
        "\t\n",
        "\t# Prediction using gini\n",
        "\ty_pred_gini = prediction(X_test, clf_gini)\n",
        "\tcal_accuracy(y_test, y_pred_gini)\n",
        "\t\n",
        "\tprint(\"Results Using Entropy:\")\n",
        "\t# Prediction using entropy\n",
        "\ty_pred_entropy = prediction(X_test, clf_entropy)\n",
        "\tcal_accuracy(y_test, y_pred_entropy)\n",
        "\t\n",
        "\t\n",
        "# Calling main function\n",
        "if __name__==\"__main__\":\n",
        "\tmain()\n"
      ],
      "metadata": {
        "id": "P7BniHI4qNf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##KNN Classification\n"
      ],
      "metadata": {
        "id": "mYSnHf3dhTbg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dgSrzaIOEsP"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y, test_size = 0.30,random_state = 17)\n",
        "#X_train, Y_test = train_test_split(df, test_size = 0.20, random_state= 17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "X_test_std = scaler.fit_transform(X_test)\n",
        "# X_train =  scaler.transform(X_train)\n",
        "# X_test  =  scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "W4QmXDbmhq2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn import utils\n",
        "\n",
        "lab_enc = preprocessing.LabelEncoder()\n",
        "encoded = lab_enc.fit_transform(Y_train)\n",
        "\n",
        "\n",
        "print(utils.multiclass.type_of_target(Y_train))\n",
        "\n",
        "print(utils.multiclass.type_of_target(Y_train.astype('int')))\n",
        "\n",
        "\n",
        "print(utils.multiclass.type_of_target(encoded))\n"
      ],
      "metadata": {
        "id": "o1Ev1xE4htXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "import seaborn as sns\n",
        "def get_knn(X_train,Y_train,X_test,Y_test,n_neighbors):\n",
        "  neigh = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
        "  neigh.fit(X_train, Y_train)\n",
        "  accuracy = accuracy_score(neigh.predict(X_test),Y_test)\n",
        "  print('Accuracy:',accuracy,'\\n\\n')\n",
        "  print(classification_report(neigh.predict(X_test),Y_test))\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "nrmL5czlhwTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_list = []\n",
        "n_neighbor_list = []\n",
        "accuracy_std_list = []\n",
        "for i in range(1,11):\n",
        "  print(\"KNN for n= \",i,\" after flattening - (1540 * 8 x 33) :\\n\")\n",
        "  accuracy = get_knn(X_train,Y_train,X_test,Y_test,n_neighbors=i)\n",
        "  print(\"KNN after standardization  for n= \",i,\":\\n\")\n",
        "  accuracy_std = get_knn(X_train_std,Y_train,X_test_std,Y_test,n_neighbors=i)\n",
        "  accuracy_list.append(accuracy)\n",
        "  n_neighbor_list.append(i)\n",
        "  accuracy_std_list.append(accuracy_std)"
      ],
      "metadata": {
        "id": "laerZ6zPh0kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM + 1D Convolution"
      ],
      "metadata": {
        "id": "FCNFwyp1kc5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #@title imports\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import networkx as nx\n",
        "# import pickle\n",
        "\n",
        "# import json\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from numpy import array\n",
        "# from numpy import hstack\n",
        "# from keras.models import Sequential\n",
        "# #from keras.layers import LSTM\n",
        "# #from keras.layers import Dense\n",
        "# #import scipy.stats as st\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# from datetime import datetime\n",
        "# from sklearn import preprocessing\n",
        "# from sklearn import metrics\n",
        "# from sklearn.metrics import classification_report\n",
        "# from sklearn.metrics import accuracy_score, confusion_matrix, adjusted_rand_score\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# #import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# #import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch.autograd import Variable\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "\n",
        "# torch.manual_seed(1)\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# print(\"device: \", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgL5nvBFkoG6",
        "outputId": "b70ac24e-da99-4ee5-c78f-a16148795ef5"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device:  cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data import\n",
        "# # from google.colab import files\n",
        "# # uploaded = files.upload()\n",
        "# !wget https://www.dropbox.com/s/xkozscpcryu871h/labels_1540_4classes_icmla_21.pck\n",
        "# !wget https://www.dropbox.com/s/ovt8g99wjcz1mvr/mvts_1540_icmla_21.pck\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "giV7SAoxkqi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #reading pickle files\n",
        "# def load(file_name):\n",
        "#     with open(file_name, 'rb') as fp:\n",
        "#         obj = pickle.load(fp)\n",
        "#     return obj\n",
        "\n",
        "\n",
        "# Sampled_inputs=load(\"mvts_1540_icmla_21.pck\")\n",
        "\n",
        "# Sampled_labels=load(\"labels_1540_4classes_icmla_21.pck\")    \n",
        "\n",
        "# # temp=Sampled_inputs[0]\n",
        "# # print(temp)\n",
        "# # df = pd.DataFrame(temp)\n",
        "# trainData=Sampled_inputs\n",
        "# trainLabel=Sampled_labels\n",
        "# trainLebel=Sampled_labels\n",
        "# print(\"trainData.shape: \",trainData.shape)\n",
        "# print(\"trainLebel.shape: \",trainLabel.shape)"
      ],
      "metadata": {
        "id": "wcTLaNqGk8Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import StandardScaler\n",
        "# sc = StandardScaler()\n",
        "# trainData_std = sc.fit_transform(trainData.reshape(trainData.shape[0]*trainData.shape[1],trainData.shape[2]))\n",
        "# trainData_std = trainData_std.reshape(trainData.shape)\n",
        "\n",
        "# # Normal Input\n",
        "# input_data = torch.from_numpy(trainData).float()\n",
        "\n",
        "# # # Standardized Input\n",
        "# # input_data = torch.from_numpy(trainData_std).float()"
      ],
      "metadata": {
        "id": "QzQSTLAdk9FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "\n",
        "# torch.manual_seed(1)\n",
        "\n",
        "# lstm = nn.LSTM(60, 60)  # Input dim is 3, output dim is 3\n",
        "# # inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
        "\n",
        "# inputs = input_data\n",
        "# hidden = (torch.randn(1, 33, 60),\n",
        "#           torch.randn(1, 33, 60))\n",
        "\n",
        "# output, hidden = lstm(inputs, hidden)\n",
        "\n",
        "# # for i in inputs:\n",
        "# #     # Step through the sequence one element at a time.\n",
        "# #     # after each step, hidden contains the hidden state.\n",
        "# #     out, hidden = lstm(i.view(1, 33, -1), hidden)"
      ],
      "metadata": {
        "id": "4czG61Tik_pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #standardization/z normalization of the univaraite time series\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# sc = StandardScaler()\n",
        "# npArrays=[]\n",
        "# for l in range(0, len(trainData)):\n",
        "#   trainData_std = sc.fit_transform(trainData[l])\n",
        "#   #trainData_std = trainData_std.astype(np.float64)\n",
        "#   #print(type(trainData_std[0][0]))\n",
        "#   npArrays.append(trainData_std)\n",
        "\n",
        "# print(type(npArrays))\n",
        "# arr = np.asarray(npArrays)\n",
        "# print(type(arr))\n",
        "# trainData=arr\n",
        "# print(\"trainData.shape: \",trainData.shape)\n",
        "# print(type(trainData))\n",
        "# print(\"trainLebel.shape: \",trainLabel.shape)\n",
        "# print(type(trainLabel))"
      ],
      "metadata": {
        "id": "nrVkh8hmlCBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model (Load Data, Train , Test)"
      ],
      "metadata": {
        "id": "5tOOe8qXlJOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new_train_data = output.detach().clone().numpy()\n",
        "\n",
        "# temptrainData=np.empty([1540,60, 33])\n",
        "# n=len(new_train_data)\n",
        "# for l in range(0, n):\n",
        "#   temp=new_train_data[l]\n",
        "#   #print(temp)\n",
        "#   #temp=np.transpose(temp)\n",
        "#   temp=temp.T\n",
        "#   # print(temp.shape)\n",
        "#   #print(temp)\n",
        "#   temptrainData[l,:,:]=temp\n",
        "#   n=n+1\n",
        "#   #np.append(temptrainData, temp)\n",
        "#   #print(temptrainData)\n",
        "\n",
        "# #print(temptrainData.shape)\n",
        "# #print(trainData.shape) \n",
        "# trainData=temptrainData\n",
        "# print(\"trainData.shape: \",trainData.shape)\n",
        "# #print(trainData[0])\n",
        "\n",
        "# temp=new_train_data[0]\n",
        "# #print(temp)\n",
        "# df = pd.DataFrame(temp)\n",
        "# #df=pd.DataFrame.from_dict(trainData)\n",
        "# trainData222=new_train_data"
      ],
      "metadata": {
        "id": "E9KyGCw7lNOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INPUT_DIM = 33\n",
        "# HIDDEN_DIM = 64\n",
        "# NUM_TS = 60\n",
        "# NUM_CLASSES = 4\n",
        "# num_layers = 1 #number of stacked lstm layers\n",
        "# hidden_size=HIDDEN_DIM"
      ],
      "metadata": {
        "id": "ovJ5uKyTlPgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class bcolors:\n",
        "#     HEADER = '\\033[95m'\n",
        "#     OKBLUE = '\\033[94m'\n",
        "#     OKCYAN = '\\033[96m'\n",
        "#     OKGREEN = '\\033[92m'\n",
        "#     WARNING = '\\033[93m'\n",
        "#     FAIL = '\\033[91m'\n",
        "#     ENDC = '\\033[0m'\n",
        "#     BOLD = '\\033[1m'\n",
        "#     UNDERLINE = '\\033[4m'"
      ],
      "metadata": {
        "id": "RNKfvUhzlTeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Model with 1D Convolution\n"
      ],
      "metadata": {
        "id": "32C612tElXrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # input_dim = 33, hidden_dim = 128, num_TS = 60, num_classes = 4\n",
        "\n",
        "# from collections import OrderedDict\n",
        "# class LSTM_MVTS_LRN(nn.Module):\n",
        "#   def __init__(self, input_dim, hidden_dim, num_classes):\n",
        "#     super(LSTM_MVTS_LRN, self).__init__()\n",
        "#     self.hidden_dim = hidden_dim\n",
        "  \n",
        "#     # nn.Conv1D(input_shape=(input_dim,NUM_TS)\n",
        "#     kernel_size = 2\n",
        "#     # self.conv1  = nn.Conv1d(1,hidden_dim,kernel_size = kernel_size)\n",
        "#     # self.conv2  = nn.Conv1d(hidden_dim,1,kernel_size = kernel_size)\n",
        "\n",
        "#     # self.conv1  = nn.Conv1d(1,hidden_dim,kernel_size = kernel_size,stride=2)\n",
        "#     # input_dim_afterConv1d = int(np.ceil(input_dim/2)) + 1\n",
        "#     # self.conv2  = nn.Conv1d(input_dim_afterConv1d,hidden_size,kernel_size = kernel_size)\n",
        "\n",
        "#     self.conv1  = nn.Conv1d(1,hidden_dim,kernel_size = kernel_size)\n",
        "#     input_dim_afterConv1d = int(input_dim - kernel_size)\n",
        "#     self.conv2  = nn.Conv1d(hidden_dim,1,kernel_size = kernel_size)\n",
        "    \n",
        "#     self.maxpool1d = nn.MaxPool1d(1)\n",
        "\n",
        "   \n",
        "    \n",
        "#     # Initialize the LSTM\n",
        "#     self.lstm = nn.LSTM(input_dim_afterConv1d, hidden_dim)\n",
        "    \n",
        "    \n",
        "#     # Initialize the RNN\n",
        "#     self.RNN = nn.RNN(input_dim_afterConv1d, hidden_dim)\n",
        "\n",
        "#       # nn.Conv1D(input_shape=(input_dim,NUM_TS))\n",
        "#       #(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_layers = 1, batch_first=True)\n",
        "\n",
        "\n",
        "#     self.hidden2class = nn.Linear(hidden_dim, num_classes)\n",
        "#   def forward(self, mvts):\n",
        "#     #input single mvts (60, 33); output class probability vector (1,4)\n",
        "\n",
        "#     mvts_3d = mvts.view(len(mvts), 1, -1) #  mvts.shape: (60, 33); len(mvts)=60; new shape (mvts_3d): (60, 1, 33); \n",
        "\n",
        "#     # 1d Convolution with ReLU activation \n",
        "#     # x = F.relu(self.conv1(mvts_3d))\n",
        "#     # x = F.relu(self.conv2(x))\n",
        "\n",
        "#     ## 1d Convolution with Maxpooling\n",
        "#     x = self.conv1(mvts_3d)\n",
        "#     x = self.maxpool1d(x)\n",
        "#     x = self.conv2(x)\n",
        "#     x = self.maxpool1d(x)\n",
        "    \n",
        "\n",
        "#     ## LSTM Layer\n",
        "\n",
        "#     # lstm_out,_ = self.lstm(x)\n",
        "#     # ##last_lstm_out = torch.cat((lstm_out[0],lstm_out[1]),dim=0).mean(0).unsqueeze(0) #(1,64)\n",
        "#     # last_lstm_out = lstm_out[len(lstm_out)-1]\n",
        "#     # final_output = last_lstm_out\n",
        "\n",
        "#     ## RNN Layer\n",
        "\n",
        "#     RNN_out,_ = self.RNN(x)\n",
        "#     # last_RNN_out = torch.cat((RNN_out[0],RNN_out[1]),dim=0).mean(0).unsqueeze(0) #(1,64)\n",
        "#     last_RNN_out = RNN_out[len(RNN_out)-1]\n",
        "#     final_output = last_RNN_out\n",
        "    \n",
        "#     # print(final_output.shape)\n",
        "#     class_space = self.hidden2class(final_output) #(1,4)\n",
        "#     class_scores = F.log_softmax(class_space, dim=1)\n",
        "#     return class_scores\n"
      ],
      "metadata": {
        "id": "RNvqk0l7lUs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dowork - Train"
      ],
      "metadata": {
        "id": "BXzu8QLJldus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import random\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def doWork( X_train, X_test, y_train, y_test):\n",
        "\n",
        "#     num_masterIteration=1\n",
        "\n",
        "#     #print(\"X_train.shape: \",X_train.shape)\n",
        "#     #print(\"X_test.shape: \",X_test.shape)\n",
        "#     #print(\"y_train.shape: \",y_train.shape)\n",
        "#     #print(\"y_test.shape: \",y_test.shape)\n",
        "#     #print(type(X_train))\n",
        "\n",
        "#     classification_report_dict=[]\n",
        "#     Accuracy=[]\n",
        "#     for masterIteration in range(num_masterIteration):\n",
        "#         print(\"\\nmasterIteration: \",masterIteration)\n",
        "#         #print(bcolors.WARNING + \"\\nmasterIteration :\" + bcolors.WARNING,masterIteration)\n",
        "#         random_state=random.randint(42, 100)\n",
        "#         #print(\"random_state: \",random_state)\n",
        "\n",
        "\n",
        "#         model = LSTM_MVTS_LRN(INPUT_DIM, HIDDEN_DIM, NUM_CLASSES)\n",
        "#         loss_function = nn.NLLLoss()\n",
        "#         optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "#         numTrain = X_train.shape[0]\n",
        "\n",
        "#         numEpochs =1\n",
        "\n",
        "#         #train\n",
        "#         for epoch in range(numEpochs):\n",
        "#           print(\"\\n nmasterIteration, epoch: \",masterIteration,epoch)\n",
        "#           loss_values = []\n",
        "#           running_loss = 0.0\n",
        "\n",
        "\n",
        "#           for i in range(numTrain):\n",
        "#             model.zero_grad()\n",
        "#             mvts = X_train[i,:,:]\n",
        "#             mvts = torch.from_numpy(mvts).float()\n",
        "\n",
        "\n",
        "#             target = y_train[i]\n",
        "#             #print(type(target))\n",
        "#             target = [target]\n",
        "#             #print(type(target))\n",
        "#             target=np.array(target)\n",
        "#             #print(type(target))\n",
        "#             target = torch.Tensor(target)\n",
        "#             #print(type(target))\n",
        "\n",
        "#             target = target.type(torch.LongTensor)\n",
        "\n",
        "#             #print(type(mvts))\n",
        "#             #print(mvts.is_cuda)\n",
        "#             mvts = mvts.to(device)\n",
        "#             #print(mvts.is_cuda)\n",
        "#             #print(type(target))\n",
        "#             #print(target.is_cuda)\n",
        "#             target = target.to(device)\n",
        "#             #print(target.is_cuda)\n",
        "\n",
        "#             #target = torch.from_numpy(np.array(target))\n",
        "#             mvts = mvts.view(mvts.size(0), -1)\n",
        "#             model.to(device)\n",
        "   \n",
        "#             class_scores = model(mvts)\n",
        "           \n",
        "#             # print(class_scores)\n",
        "#             #print(target)\n",
        "\n",
        "#             loss = loss_function(class_scores, target)\n",
        "                     \n",
        "#             #loss = criterion_label(outputs, labels)\n",
        "#             #print(loss)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             running_loss += loss.item()\n",
        "\n",
        "#             #loss_values.append(running_loss)\n",
        "#             loss_values.append(running_loss / len(X_train))\n",
        "            \n",
        "#           maxAcc=0\n",
        "#           max_classification_report_dict=0\n",
        "#           #print(\"loss: \",loss)  \n",
        "#           #test \n",
        "#           numTest = X_test.shape[0]\n",
        "#           with torch.no_grad():\n",
        "#               numCorrect = 0\n",
        "#               testLabel=[]\n",
        "#               predictaedLabel=[]\n",
        "#               for i in range(numTest):\n",
        "#                 test_mvts = X_test[i,:,:]\n",
        "#                 test_label = y_test[i] #class = 2\n",
        "\n",
        "#                 #model.zero_grad()\n",
        "\n",
        "#                 test_mvts = torch.from_numpy(test_mvts).float()\n",
        "\n",
        "#                 test_mvts = test_mvts.to(device)\n",
        "#                 #test_label = test_label.to(device)\n",
        "\n",
        "#                 #print(\"\\n\\n1.test_label: \", test_label)\n",
        "#                 test_class_scores = model(test_mvts) #test mvts = [0.35, 0.15, 0.45, 0.05]\n",
        "#                 #print(\"2.1 test_class_scores: \",test_class_scores)\n",
        "#                 #print(\" type(test_class_scores): \",type(test_class_scores)) \n",
        "#                 class_prediction = torch.argmax(test_class_scores, dim=-1) #2\n",
        "#                 #print(\"2.2 class_prediction: \", class_prediction)\n",
        "\n",
        "#                 current_seq = np.argmax(test_class_scores.cpu().numpy())\n",
        "#                 #print(\"4 current_seq: \",current_seq)\n",
        "#                 #print(\"4 np.argmax(current_seq): \",current_seq))\n",
        "\n",
        "#                 #print(\"3.0 class_prediction == test_label:\", class_prediction == test_label)\n",
        "#                 testLabel.append(test_label)\n",
        "#                 predictaedLabel.append(current_seq)\n",
        "\n",
        "\n",
        "\n",
        "#                 if(class_prediction == test_label): #(2,3 ) match \n",
        "#                   numCorrect = numCorrect+1\n",
        "#               acc = numCorrect/numTest\n",
        "#               #print(\"acc:\", acc) \n",
        "\n",
        "#               #print(\"testLabel[]: \",testLabel)\n",
        "#               #print(\"predictaedLabel[]: \",predictaedLabel)\n",
        "\n",
        "#               fgdg=round(acc, 2)\n",
        "#               #print(\"fgdg:\", fgdg) \n",
        "#               if fgdg  > maxAcc:\n",
        "#                 maxAcc=acc\n",
        "#                 print(bcolors.WARNING + \"maxAcc:\" + bcolors.ENDC,maxAcc)\n",
        "#                 max_classification_report_dict=metrics.classification_report(testLabel, predictaedLabel, digits=3,output_dict=True)\n",
        "#         print(np.array(loss_values))\n",
        "#         plt.plot(np.array(loss_values), 'r')\n",
        "#         plt.xlabel('Epochs')\n",
        "#         plt.ylabel('Total_Loss')\n",
        "#         classification_report_dict.append(max_classification_report_dict)   \n",
        "#         #print('classification_report_dict : \\n',classification_report_dict)\n",
        "#         Accuracy.append(maxAcc)  \n",
        "#         #print('Accuracy : \\n',Accuracy)\n",
        "#     mean_acc = doClassSpecificCalulcation(Accuracy,trainLebel,classification_report_dict)\n",
        "#     return mean_acc\n"
      ],
      "metadata": {
        "id": "4_6_NSrWlbnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def doClassSpecificCalulcation(Accuracy,trainLebel,classification_report_dict):\n",
        "#   mean_acc = np.mean(Accuracy)\n",
        "#   print('\\np.mean(Accuracy) :',np.mean(Accuracy))\n",
        "#   print('\\np.std(Accuracy) :',np.std(Accuracy))\n",
        "#   print('\\n33333333 p.mean np.std(Accuracy) :     ',np.round(np.mean(Accuracy),2),\"+-\",np.round(np.std(Accuracy),2) )\n",
        "#   for j in range( len(np.unique(trainLebel)) ):\n",
        "#     print('\\n\\n\\n\\nclass :',j) \n",
        "#     precision=[]\n",
        "#     recall=[]\n",
        "#     f1_score=[]\n",
        "#     for i in range(len(classification_report_dict)):\n",
        "#       report=classification_report_dict[i]\n",
        "#       #print('classification_report : \\n',report) \n",
        "#       temp=report[str(j)]['precision'] \n",
        "#       precision.append(temp)\n",
        "\n",
        "#       temp=report[str(j)]['recall'] \n",
        "#       recall.append(temp)\n",
        "\n",
        "#       temp=report[str(j)]['f1-score'] \n",
        "#       f1_score.append(temp)\n",
        "\n",
        "#     print('\\np.mean(precision) \\t p.mean(recall) \\t p.mean(f1_score) :') \n",
        "\n",
        "\n",
        "#     print(np.mean(precision)) \n",
        "#     print(np.mean(recall)) \n",
        "#     print(np.mean(f1_score))\n",
        "\n",
        "#     print('\\np.mean p.std(precision) \\tp.mean  p.std(recall) \\tp.mean  p.std(f1_score) :')\n",
        "\n",
        "#     print(np.round(np.mean(precision),2),\"+-\",np.round(np.std(precision),2) )\n",
        "#     print(np.round(np.mean(recall),2),\"+-\",np.round(np.std(recall),2) )\n",
        "#     print(np.round(np.mean(f1_score),2),\"+-\",np.round(np.std(f1_score),2) )\n",
        "#   return mean_acc"
      ],
      "metadata": {
        "id": "k80TJFCmliiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def doWork2():\n",
        "\n",
        "#   #test_sizes=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
        "#   test_sizes=[0.3]\n",
        "#   for temp4 in range(len(test_sizes)):\n",
        "#     test_size=test_sizes[temp4]\n",
        "#     print(\"\\n\\n\\n *************** test_size: \",test_size)\n",
        "#     random_state=random.randint(42, 100)\n",
        "#     print(\"random_state: \",random_state)\n",
        "    \n",
        "#     X_train, X_test, y_train, y_test = train_test_split(trainData, trainLebel, test_size=test_size, random_state=random_state)\n",
        "#     #print(\"X_train.shape: \",X_train.shape)\n",
        "#     #print(\"X_test.shape: \",X_test.shape)\n",
        "#     #print(\"y_train.shape: \",y_train.shape)\n",
        "#     #print(\"y_test.shape: \",y_test.shape)\n",
        "#     #print(type(X_train))\n",
        "\n",
        "\n",
        "\n",
        "#     print(\"X_train.shape X_test.shape y_train.shape y_test.shape \",\n",
        "#               X_train.shape, X_test.shape ,y_train.shape, y_test.shape)\n",
        "#     mean_acc = doWork( X_train, X_test, y_train, y_test)\n",
        "#   return mean_acc"
      ],
      "metadata": {
        "id": "eVBiSLiHlmf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Run Main (doWork2)**"
      ],
      "metadata": {
        "id": "FKa8l6Znlra4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#doWork2()"
      ],
      "metadata": {
        "id": "qBYLO7whlsFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original Model"
      ],
      "metadata": {
        "id": "GHY6bsRvlzD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class LSTM_MVTS_LRN(nn.Module):\n",
        "#   def __init__(self, input_dim, hidden_dim, num_classes):\n",
        "#     super(LSTM_MVTS_LRN, self).__init__()\n",
        "#     self.hidden_dim = hidden_dim\n",
        "      \n",
        "#     #self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
        "#     # Initialize the RNN.\n",
        "#     self.lstm = nn.RNN(input_dim, hidden_dim)\n",
        "#       #(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_layers = 1, batch_first=True)\n",
        "\n",
        "#     self.hidden2class = nn.Linear(hidden_dim, num_classes)\n",
        "#   def forward(self, mvts):\n",
        "#     #print(mvts.shape)\n",
        "#     #input single mvts (60, 33); output class probability vector (1,4)\n",
        "#     lstm_out, _ = self.lstm(mvts.view(len(mvts), 1, -1)) #mvts.shape: (60, 33); len(mvts)=60; new shape: (60, 1, 33); lstm_out --> (60, 128)\n",
        "#     last_lstm_out = lstm_out[len(lstm_out)-1] #(1,128)\n",
        "#     class_space = self.hidden2class(last_lstm_out) #(1,4)\n",
        "#     class_scores = F.log_softmax(class_space, dim=1)\n",
        "#     return class_scores"
      ],
      "metadata": {
        "id": "EXMVEZf-lulr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dowork - Train"
      ],
      "metadata": {
        "id": "4mmAZ0dGl4c-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import random\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def doWork( X_train, X_test, y_train, y_test):\n",
        "\n",
        "#     num_masterIteration=1\n",
        "\n",
        "#     #print(\"X_train.shape: \",X_train.shape)\n",
        "#     #print(\"X_test.shape: \",X_test.shape)\n",
        "#     #print(\"y_train.shape: \",y_train.shape)\n",
        "#     #print(\"y_test.shape: \",y_test.shape)\n",
        "#     #print(type(X_train))\n",
        "\n",
        "#     classification_report_dict=[]\n",
        "#     Accuracy=[]\n",
        "#     for masterIteration in range(num_masterIteration):\n",
        "#         print(\"\\nmasterIteration: \",masterIteration)\n",
        "#         #print(bcolors.WARNING + \"\\nmasterIteration :\" + bcolors.WARNING,masterIteration)\n",
        "#         random_state=random.randint(42, 100)\n",
        "#         #print(\"random_state: \",random_state)\n",
        "\n",
        "\n",
        "#         model = LSTM_MVTS_LRN(INPUT_DIM, HIDDEN_DIM, NUM_CLASSES)\n",
        "#         loss_function = nn.NLLLoss()\n",
        "#         optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "#         numTrain = X_train.shape[0]\n",
        "\n",
        "#         numEpochs =1\n",
        "\n",
        "#         #train\n",
        "#         for epoch in range(numEpochs):\n",
        "#           print(\"\\n nmasterIteration, epoch: \",masterIteration,epoch)\n",
        "#           loss_values = []\n",
        "#           running_loss = 0.0\n",
        "\n",
        "\n",
        "#           for i in range(numTrain):\n",
        "#             model.zero_grad()\n",
        "#             mvts = X_train[i,:,:]\n",
        "#             mvts = torch.from_numpy(mvts).float()\n",
        "\n",
        "\n",
        "#             target = y_train[i]\n",
        "#             #print(type(target))\n",
        "#             target = [target]\n",
        "#             #print(type(target))\n",
        "#             target=np.array(target)\n",
        "#             #print(type(target))\n",
        "#             target = torch.Tensor(target)\n",
        "#             #print(type(target))\n",
        "\n",
        "#             target = target.type(torch.LongTensor)\n",
        "\n",
        "#             #print(type(mvts))\n",
        "#             #print(mvts.is_cuda)\n",
        "#             mvts = mvts.to(device)\n",
        "#             #print(mvts.is_cuda)\n",
        "\n",
        "#             #print(type(target))\n",
        "#             #print(target.is_cuda)\n",
        "#             target = target.to(device)\n",
        "#             #print(target.is_cuda)\n",
        "\n",
        "#             #target = torch.from_numpy(np.array(target))\n",
        "#             mvts = mvts.view(mvts.size(0), -1)\n",
        "#             model.to(device)\n",
        "   \n",
        "#             class_scores = model(mvts)\n",
        "#             # print(class_scores)\n",
        "#             #print(target)\n",
        "\n",
        "#             loss = loss_function(class_scores, target)\n",
        "#             #loss = criterion_label(outputs, labels)\n",
        "#             #print(loss)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             running_loss += loss.item()\n",
        "\n",
        "#             #loss_values.append(running_loss)\n",
        "#             loss_values.append(running_loss / len(X_train))\n",
        "\n",
        "#           maxAcc=0\n",
        "#           max_classification_report_dict=0\n",
        "#           #print(\"loss: \",loss)  \n",
        "#           #test \n",
        "#           numTest = X_test.shape[0]\n",
        "#           with torch.no_grad():\n",
        "#               numCorrect = 0\n",
        "#               testLabel=[]\n",
        "#               predictaedLabel=[]\n",
        "#               for i in range(numTest):\n",
        "#                 test_mvts = X_test[i,:,:]\n",
        "#                 test_label = y_test[i] #class = 2\n",
        "\n",
        "#                 #model.zero_grad()\n",
        "\n",
        "#                 test_mvts = torch.from_numpy(test_mvts).float()\n",
        "\n",
        "#                 test_mvts = test_mvts.to(device)\n",
        "#                 #test_label = test_label.to(device)\n",
        "\n",
        "#                 #print(\"\\n\\n1.test_label: \", test_label)\n",
        "#                 test_class_scores = model(test_mvts) #test mvts = [0.35, 0.15, 0.45, 0.05]\n",
        "#                 #print(\"2.1 test_class_scores: \",test_class_scores)\n",
        "#                 #print(\" type(test_class_scores): \",type(test_class_scores)) \n",
        "#                 class_prediction = torch.argmax(test_class_scores, dim=-1) #2\n",
        "#                 #print(\"2.2 class_prediction: \", class_prediction)\n",
        "\n",
        "#                 current_seq = np.argmax(test_class_scores.cpu().numpy())\n",
        "#                 #print(\"4 current_seq: \",current_seq)\n",
        "#                 #print(\"4 np.argmax(current_seq): \",current_seq))\n",
        "\n",
        "#                 #print(\"3.0 class_prediction == test_label:\", class_prediction == test_label)\n",
        "#                 testLabel.append(test_label)\n",
        "#                 predictaedLabel.append(current_seq)\n",
        "\n",
        "\n",
        "\n",
        "#                 if(class_prediction == test_label): #(2,3 ) match \n",
        "#                   numCorrect = numCorrect+1\n",
        "#               acc = numCorrect/numTest\n",
        "#               #print(\"acc:\", acc) \n",
        "\n",
        "#               #print(\"testLabel[]: \",testLabel)\n",
        "#               #print(\"predictaedLabel[]: \",predictaedLabel)\n",
        "\n",
        "#               fgdg=round(acc, 2)\n",
        "#               #print(\"fgdg:\", fgdg) \n",
        "#               if fgdg  > maxAcc:\n",
        "#                 maxAcc=acc\n",
        "#                 print(bcolors.WARNING + \"maxAcc:\" + bcolors.ENDC,maxAcc)\n",
        "#                 max_classification_report_dict=metrics.classification_report(testLabel, predictaedLabel, digits=3,output_dict=True)\n",
        "\n",
        "#         plt.plot(np.array(loss_values), 'r')\n",
        "#         plt.xlabel('Epochs')\n",
        "#         plt.ylabel('Total_Loss')\n",
        "#         classification_report_dict.append(max_classification_report_dict)   \n",
        "#         #print('classification_report_dict : \\n',classification_report_dict)\n",
        "#         Accuracy.append(maxAcc)  \n",
        "#         #print('Accuracy : \\n',Accuracy)\n",
        "#     mean_acc = doClassSpecificCalulcation(Accuracy,trainLebel,classification_report_dict)\n",
        "#     return mean_acc\n"
      ],
      "metadata": {
        "id": "O5XKi7BSl9w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def doClassSpecificCalulcation(Accuracy,trainLebel,classification_report_dict):\n",
        "#   mean_acc = np.mean(Accuracy)\n",
        "#   print('\\np.mean(Accuracy) :',np.mean(Accuracy))\n",
        "#   print('\\np.std(Accuracy) :',np.std(Accuracy))\n",
        "#   print('\\n33333333 p.mean np.std(Accuracy) :     ',np.round(np.mean(Accuracy),2),\"+-\",np.round(np.std(Accuracy),2) )\n",
        "#   for j in range( len(np.unique(trainLebel)) ):\n",
        "#     print('\\n\\n\\n\\nclass :',j) \n",
        "#     precision=[]\n",
        "#     recall=[]\n",
        "#     f1_score=[]\n",
        "#     for i in range(len(classification_report_dict)):\n",
        "#       report=classification_report_dict[i]\n",
        "#       #print('classification_report : \\n',report) \n",
        "#       temp=report[str(j)]['precision'] \n",
        "#       precision.append(temp)\n",
        "\n",
        "#       temp=report[str(j)]['recall'] \n",
        "#       recall.append(temp)\n",
        "\n",
        "#       temp=report[str(j)]['f1-score'] \n",
        "#       f1_score.append(temp)\n",
        "\n",
        "#     print('\\np.mean(precision) \\t p.mean(recall) \\t p.mean(f1_score) :') \n",
        "\n",
        "\n",
        "#     print(np.mean(precision)) \n",
        "#     print(np.mean(recall)) \n",
        "#     print(np.mean(f1_score))\n",
        "\n",
        "#     print('\\np.mean p.std(precision) \\tp.mean  p.std(recall) \\tp.mean  p.std(f1_score) :')\n",
        "\n",
        "#     print(np.round(np.mean(precision),2),\"+-\",np.round(np.std(precision),2) )\n",
        "#     print(np.round(np.mean(recall),2),\"+-\",np.round(np.std(recall),2) )\n",
        "#     print(np.round(np.mean(f1_score),2),\"+-\",np.round(np.std(f1_score),2) )\n",
        "#   return mean_acc"
      ],
      "metadata": {
        "id": "7E3B03sTmCpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def doWork2():\n",
        "\n",
        "#   #test_sizes=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
        "#   test_sizes=[0.3]\n",
        "#   for temp4 in range(len(test_sizes)):\n",
        "#     test_size=test_sizes[temp4]\n",
        "#     print(\"\\n\\n\\n *************** test_size: \",test_size)\n",
        "#     random_state=random.randint(42, 100)\n",
        "#     print(\"random_state: \",random_state)\n",
        "    \n",
        "#     X_train, X_test, y_train, y_test = train_test_split(trainData, trainLebel, test_size=test_size, random_state=random_state)\n",
        "#     #print(\"X_train.shape: \",X_train.shape)\n",
        "#     #print(\"X_test.shape: \",X_test.shape)\n",
        "#     #print(\"y_train.shape: \",y_train.shape)\n",
        "#     #print(\"y_test.shape: \",y_test.shape)\n",
        "#     #print(type(X_train))\n",
        "\n",
        "\n",
        "\n",
        "#     print(\"X_train.shape X_test.shape y_train.shape y_test.shape \",\n",
        "#               X_train.shape, X_test.shape ,y_train.shape, y_test.shape)\n",
        "#     mean_acc = doWork( X_train, X_test, y_train, y_test)\n",
        "#   return mean_acc"
      ],
      "metadata": {
        "id": "Qujzs2KFmGHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Run Main (doWork2)**"
      ],
      "metadata": {
        "id": "7-ryJ2E4mJBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# doWork2()"
      ],
      "metadata": {
        "id": "VtNCrrqnmHCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking if results are consistent : **STD DEV across 20 runs**\n"
      ],
      "metadata": {
        "id": "xak9VNBamTqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list_mean_acc = []\n",
        "# for _ in range(20):\n",
        "#   mean_acc = doWork2()\n",
        "#   list_mean_acc.append(mean_acc)"
      ],
      "metadata": {
        "id": "wZIcD6rgmOlu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}